{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPw4nqIXOuR8"
   },
   "source": [
    "# importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbK57ve40rYo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df287cf85243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGbv-ujyO1ur"
   },
   "source": [
    "# hyperparameter  setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYLMh50fPJ_6"
   },
   "source": [
    "# Data generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8RRF73gPG0m"
   },
   "outputs": [],
   "source": [
    "# 产生一长串卷积数据，然后切割成batch (不考虑state memory，即由keras自动划分epoch)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(15)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "#epochs=3\n",
    "#sample_epoch=2000\n",
    "\n",
    "#samples=batch_size*epochs\n",
    "#samples=5000                                             # train_set size=samples*seq_len/train_test_ratio  train_test_ratio=3 \n",
    "seq_len=10                                            # time-step size\n",
    "lstm_size=100\n",
    "dropout=0\n",
    "\n",
    "\n",
    "epochs_step=1000\n",
    "data_reps=5\n",
    "\n",
    "batch_size=500\n",
    "shuffle_option=False\n",
    "\n",
    "\n",
    "\n",
    "train_sample=5000\n",
    "train_test_ratio=1\n",
    "\n",
    "datasize=train_sample*train_test_ratio\n",
    "\n",
    "samples_iter=1000\n",
    "\n",
    "snr_db=20\n",
    "\n",
    "\n",
    "#datasize=seq_len*samples\n",
    "#datasize=10\n",
    "\n",
    "modulation=2                #modulation^2 为 QAM星座点数\n",
    "\n",
    "h0=np.array([0.5 ], dtype=complex)                            #awgn\n",
    "h1=np.array([0.97, 0.23, 0.45, 0.11  ], dtype=complex)\n",
    "h2=np.array([0.3482, 0.8704, 0.3482  ], dtype=complex)  # Linear non-minimum phase channel.\n",
    "h4=np.array([1.22 + 1j*0.646,    0.063 - 1j*0.001,     -0.024 - 1j*0.014,    0.036 + 1j*0.031 ],dtype=complex)  #from Adaptive Decision Feedback Equalization for Digital\n",
    "h3=np.array([1, -2, 1   ], dtype=complex)                # partial response channel with double zero on the unit circle\n",
    "\n",
    "channel_h=h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZDtA8xjhxUI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "source_0=np.random.randint(0,modulation,datasize*2)            #16qam     0 1 2 3\n",
    "source=np.array(source_0)\n",
    "#print(source)\n",
    "#source=source*2-3                                       # -3 -1 1 3\n",
    "source=source*2-(modulation-1)                          # -1 1\n",
    "source_0=np.reshape(source_0,[-1,2])                  # split into real and imag part\n",
    "source=np.reshape(source,[-1,2])                      # split into real and imag part\n",
    "#print(source)\n",
    "print(\"source code size is:\" , source.shape)\n",
    "#print(source[0:5,:])\n",
    "\n",
    "#out_demod=source_0[:,0]*4+source_0[:,1]            # complex modulation to 0-15 constellation point\n",
    "out_demod=source_0[:,0]*modulation+source_0[:,1]            # complex modulation to 0-15 constellation point\n",
    "#print(out_demod)\n",
    "print(\"out_demod len=\",len(out_demod))\n",
    "out_demod_cat=np.zeros((len(out_demod),16))\n",
    "\n",
    "for i in range(len(out_demod)):\n",
    "  out_demod_cat[i,out_demod[i]]=1\n",
    "\n",
    "print(\"out_demod_cat size\",out_demod_cat.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(out_demod)\n",
    "\n",
    "#out_demod=np.reshape(out_demod,[samples,-1])\n",
    "#print(\"out_demod shape=\",out_demod.shape)\n",
    "#print(out_demod)\n",
    "#out_demod=sequence.pad_sequences(out_demod, seq_len+3, padding='post', truncating='pre', value=16.0)\n",
    "#print(out_demod)\n",
    "#print(\"out_demod size is:\" , out_demod.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAQfHVOiFsCA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYTXDIGtGLIH"
   },
   "outputs": [],
   "source": [
    "source_z=source[:,0]+1j*source[:,1]\n",
    "\n",
    "\n",
    "snr=10**(snr_db/10)\n",
    "\n",
    "\n",
    "receive_z_1=np.convolve(source_z, channel_h,'same')        # 一整串卷积 然后切割\n",
    "#receive_z_1=receive_z_1+0.2*(receive_z_1**2)  # 非线性信道\n",
    "\n",
    "#print(receive_z_1[0:20])\n",
    "\n",
    "power=np.square(np.absolute(receive_z_1))                  # 对输入采样进行归一化\n",
    "#print(power[0:20])\n",
    "\n",
    "\n",
    "signal_power=np.average(power)                            #添加噪声\n",
    "print(\"signal power=\", signal_power)\n",
    "\n",
    "noise_oneside_power=signal_power/2/snr\n",
    "#print(noise_oneside_power)\n",
    "noise=np.random.normal(0, noise_oneside_power**0.5, receive_z_1.shape)+1j*np.random.normal(0, noise_oneside_power**0.5, receive_z_1.shape)\n",
    "#noise=np.random.normal(0, 1, receive_z_1.shape)+1j*np.random.normal(0, 1, receive_z_1.shape)\n",
    "#print(noise[0:20])\n",
    "print(\"complex noise average =\",np.average(noise))\n",
    "print(\"complex noise power =\", np.average(np.square(np.absolute(noise))))\n",
    "print(\"snr\", signal_power/(np.average(np.square(np.absolute(noise)))) )\n",
    "\n",
    "receive_z_1_with_noise=receive_z_1+noise\n",
    "print(\"  recieved no noise\", receive_z_1[0:4])\n",
    "print(\"recieved with noise\", receive_z_1_with_noise[0:4])\n",
    "\n",
    "print(\"              noise\", receive_z_1_with_noise[0:4]-receive_z_1[0:4])\n",
    "\n",
    "power_with_noise=np.square(np.absolute(receive_z_1_with_noise))                  # 对输入采样进行归一化 scaling\n",
    "#print(power[0:20])\n",
    "\n",
    "scale=(np.amax(power_with_noise))**0.5\n",
    "\n",
    "print(\"scaling factor=\", scale)\n",
    "\n",
    "\n",
    "receive_z_1_with_noise=receive_z_1_with_noise/scale\n",
    "\n",
    "#receive_z_1=np.reshape(receive_z_1,[samples,-1])\n",
    "\n",
    "# 这个方法暂时不用了，就使用上面的整串卷积\n",
    "# source_z2=np.reshape(source_z,[samples,-1])       # 先切割成片段 然后卷积  \n",
    "# receive_z_2=[]\n",
    "# #print(source_z2.shape[0])\n",
    "# for l in range(source_z2.shape[0]):\n",
    "#   temp=np.convolve(source_z2[l,:],channel_h,'same')\n",
    "#   receive_z_2=np.append(receive_z_2, temp)\n",
    "\n",
    "\n",
    "# print(receive_z_1)\n",
    "# print(receive_z_2.shape)\n",
    "# print(receive_z_1-receive_z_2)\n",
    "\n",
    "\n",
    "\n",
    "receive_z=receive_z_1_with_noise\n",
    "#print(\"receive_z = \", receive_z)\n",
    "print(\"receive_z shape= \", receive_z.shape)\n",
    "\n",
    "input_temp=np.array((receive_z.real, receive_z.imag)).T         # 分成实部虚部\n",
    "#print(\"input_temp = \",input_temp)\n",
    "print(\"input_temp shape= \",input_temp.shape)\n",
    "#print(np.append([np.zeros((seq_len,2)),np.zeros((seq_len,2))],input_temp[20:(seq_len+20),:].reshape(1,seq_len,2),axis=0))\n",
    "\n",
    "\n",
    "samples=input_temp.shape[0]-seq_len           # N samples 只能产生 N-seq_len 的训练数据，除非在samples后面填0\n",
    "print(\"samples length=\", samples)\n",
    "input_z=np.zeros((seq_len,2))\n",
    "#input_z=np.empty([seq_len,2])\n",
    "input_z=input_z.reshape(1,seq_len,2)\n",
    "\n",
    "for i in range(samples):\n",
    "  #input_z[i,:,:]=input_temp[i:(seq_len+i),:].reshape(1,seq_len,2)\n",
    "  input_z=np.append(input_z, input_temp[i:(seq_len+i),:].reshape(1,seq_len,2), axis=0)\n",
    "  #print(input_z)\n",
    "  \n",
    "input_z=input_z[1:,:,:]\n",
    "#input_z=np.reshape(input_z,[samples,seq_len,2])\n",
    "#print(\"input_z = \",input_z)\n",
    "print(\"input_z shape= \",input_z.shape)\n",
    "\n",
    "\n",
    "\n",
    "out_shift=seq_len//2+1                           #单个输出，与偏移后输入符号做判决，偏移量，一般选中间位置\n",
    "out_demod=out_demod[(0+out_shift):(samples+out_shift),]                         # [8]      1位输出  sparse_categorical_crossentropy\n",
    "print(\"out_demod size =\", out_demod.shape)\n",
    "#out_demod_cat=out_demod_cat[0:samples,]                # [0,1,0,0,0.....,0]  16位输出     'categorical_crossentropy\n",
    "\n",
    "#print(\"out_demod_cat size\",out_demod_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kytMQhWR4xcO"
   },
   "outputs": [],
   "source": [
    "input_z = input_z\n",
    "y_output=out_demod\n",
    "\n",
    "#train_test_ratio=10\n",
    "X_train = input_z[0:samples//train_test_ratio,:,:]\n",
    "#X_test = input_z[samples//train_test_ratio:,:,:]        #第一组数据全部作为test 数据\n",
    "X_test0 = input_z\n",
    "\n",
    "y_train = y_output[0:samples//train_test_ratio]\n",
    "#y_test  = y_output[samples//train_test_ratio:]\n",
    "y_test0  = y_output\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test0.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aiUxGJXPVeB"
   },
   "source": [
    "# model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqSJIyaC6ie8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstm_size, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=dropout))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "#model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(modulation**2, activation='softmax'))\n",
    "#model.add((Dense(16)))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  #sparse_categorical_accuracy\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JimzYG0rPccH"
   },
   "source": [
    "# trainingg and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFi5if9A1cIJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_sample=samples_iter\n",
    "train_test_ratio=2\n",
    "datasize=train_sample*train_test_ratio\n",
    "\n",
    "data_set=0\n",
    "for i in range(data_reps):\n",
    "  print(\" SNR in db:\", snr_db)\n",
    "  print(\"modulation:\", modulation**2)\n",
    "  print(\"channel im:\", channel_h)\n",
    "  print(\"LSTM size :\", lstm_size)\n",
    "  print(\"batchsize :\", batch_size)\n",
    "  print(\"   dropout:\", dropout)\n",
    "  print(\"===================================================================================\")\n",
    "  \n",
    "  source_0=np.random.randint(0,modulation,datasize*2)            #16qam     0 1 2 3\n",
    "  source=np.array(source_0)\n",
    "  source=source*2-(modulation-1)                          # -1 1\n",
    "  source_0=np.reshape(source_0,[-1,2])                  # split into real and imag part\n",
    "  source=np.reshape(source,[-1,2])                      # split into real and imag part\n",
    "  out_demod=source_0[:,0]*modulation+source_0[:,1]            # complex modulation to 0-15 constellation point\n",
    "  out_demod_cat=np.zeros((len(out_demod),16))\n",
    "\n",
    "  for i in range(len(out_demod)):\n",
    "    out_demod_cat[i,out_demod[i]]=1\n",
    "\n",
    "  source_z=source[:,0]+1j*source[:,1]\n",
    "\n",
    "  receive_z_1=np.convolve(source_z, channel_h,'same')        # 一整串卷积 然后切割\n",
    "  #receive_z_1=receive_z_1+0.2*(receive_z_1**2)  # 非线性信道\n",
    "\n",
    "\n",
    "  power=np.square(np.absolute(receive_z_1))                  # 对输入采样进行归一化\n",
    "\n",
    "\n",
    "\n",
    "  signal_power=np.average(power)                            #添加噪声\n",
    "\n",
    "\n",
    "  noise_oneside_power=signal_power/2/snr\n",
    "\n",
    "  noise=np.random.normal(0, noise_oneside_power**0.5, receive_z_1.shape)+1j*np.random.normal(0, noise_oneside_power**0.5, receive_z_1.shape)\n",
    "\n",
    "#   print(\"complex noise average =\",np.average(noise))\n",
    "#   print(\"complex noise power =\", np.average(np.square(np.absolute(noise))))\n",
    "#   print(\"snr\", signal_power/(np.average(np.square(np.absolute(noise)))) )\n",
    "\n",
    "  receive_z_1_with_noise=receive_z_1+noise\n",
    "#   print(receive_z_1_with_noise[0:4])\n",
    "#   print(receive_z_1_with_noise[0:4]-receive_z_1[0:4])\n",
    "\n",
    "  power_with_noise=np.square(np.absolute(receive_z_1_with_noise))                  # 对输入采样进行归一化 scaling\n",
    "\n",
    "\n",
    "  #scale=(np.amax(power_with_noise))**0.5\n",
    "  receive_z_1_with_noise=receive_z_1_with_noise/scale\n",
    "  receive_z=receive_z_1_with_noise\n",
    "\n",
    "  input_temp=np.array((receive_z.real, receive_z.imag)).T         # 分成实部虚部\n",
    "\n",
    "\n",
    "\n",
    "  samples=input_temp.shape[0]-seq_len           # N samples 只能产生 N-seq_len 的训练数据，除非在samples后面填0\n",
    "\n",
    "  input_z=np.zeros((seq_len,2))\n",
    "\n",
    "  input_z=input_z.reshape(1,seq_len,2)\n",
    "\n",
    "  for i in range(samples):\n",
    "    input_z=np.append(input_z, input_temp[i:(seq_len+i),:].reshape(1,seq_len,2), axis=0)\n",
    "\n",
    "  input_z=input_z[1:,:,:]\n",
    "\n",
    "  out_shift=seq_len//2+1                           #单个输出，与偏移后输入符号做判决，偏移量，一般选中间位置\n",
    "  out_demod=out_demod[(0+out_shift):(samples+out_shift),]                         # [8]      1位输出  sparse_categorical_crossentropy\n",
    "\n",
    "  input_z = input_z\n",
    "  y_output=out_demod\n",
    "\n",
    "  X_train = input_z[0:samples//train_test_ratio,:,:]\n",
    "  X_test = input_z[samples//train_test_ratio:,:,:]        \n",
    "  \n",
    "\n",
    "  y_train = y_output[0:samples//train_test_ratio]\n",
    "  y_test  = y_output[samples//train_test_ratio:]\n",
    "\n",
    "\n",
    "  model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=epochs_step, batch_size=batch_size, shuffle=shuffle_option, verbose=1)\n",
    "\n",
    "\n",
    "  # Final evaluation of the model\n",
    "  scores = model.evaluate(X_test0, y_test0, verbose=1)\n",
    "  #scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "  print(\"========================================================= \\n  Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "  data_set+=1\n",
    "  print(\"  data set: %d / %d \" %(data_set,data_reps))\n",
    "#   print(\" SNR in db:\", snr_db)\n",
    "#   print(\"===================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5Q44adbYHGK"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=epochs_step, batch_size=batch_size, shuffle=shuffle_option)\n",
    "\n",
    "\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwMEybLCfHAM"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, batch_size=20)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7NRhlTggp32"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iR-VU6wtgqSw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFUaPX0kjoue"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lrcsS-SFtZW"
   },
   "outputs": [],
   "source": [
    "#print(X_test[0:4,:,:].shape)\n",
    "print(np.argmax(model.predict(X_test[1:56,:,:]),axis=1))\n",
    "print(y_test[1:56])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "equalizer_lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
